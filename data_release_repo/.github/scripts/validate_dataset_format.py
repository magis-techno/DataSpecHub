#!/usr/bin/env python3
"""
éªŒè¯è®­ç»ƒæ•°æ®é›†JSONæ–‡ä»¶æ ¼å¼
"""

import json
import os
import sys
from typing import Dict, List, Any
from jsonschema import validate, ValidationError

# è®­ç»ƒæ•°æ®é›†JSON Schema
TRAINING_DATASET_SCHEMA = {
    "type": "object",
    "required": ["meta", "dataset_index"],
    "properties": {
        "meta": {
            "type": "object",
            "required": ["release_name", "consumer_version", "bundle_versions", "created_at", "description", "version"],
            "properties": {
                "release_name": {"type": "string"},
                "consumer_version": {"type": "string", "pattern": r"^v\d+\.\d+\.\d+"},
                "bundle_versions": {
                    "type": "array",
                    "items": {"type": "string", "pattern": r"^v\d+\.\d+\.\d+-\d{8}-\d{6}$"}
                },
                "created_at": {"type": "string"},
                "description": {"type": "string"},
                "version": {"type": "string", "pattern": r"^v\d+\.\d+\.\d+"},
                "status": {"type": "string", "enum": ["pending", "producing", "produced", "completed", "failed"]},
                "produced_at": {"type": "string"},
                "training_type": {"type": "string", "enum": ["regular", "dagger"]}
            }
        },
        "dataset_index": {
            "type": "array",
            "items": {
                "type": "object",
                "required": ["name", "obs_path", "bundle_versions", "duplicate"],
                "properties": {
                    "name": {"type": "string"},
                    "obs_path": {"type": "string"},
                    "bundle_versions": {
                        "type": "array",
                        "items": {"type": "string"}
                    },
                    "duplicate": {"type": "integer", "minimum": 1},
                    "status": {"type": "string", "enum": ["pending", "producing", "produced", "failed"]},
                    "produced_at": {"type": "string"}
                }
            }
        }
    }
}

def find_dataset_files() -> List[str]:
    """æŸ¥æ‰¾æ‰€æœ‰è®­ç»ƒæ•°æ®é›†JSONæ–‡ä»¶"""
    dataset_files = []
    
    # æŸ¥æ‰¾æ ¹ç›®å½•ä¸‹çš„æ•°æ®é›†æ–‡ä»¶
    for filename in ['training_dataset.json', 'training_dataset.dagger.json']:
        if os.path.exists(filename):
            dataset_files.append(filename)
    
    # æŸ¥æ‰¾å­ç›®å½•ä¸­çš„æ•°æ®é›†æ–‡ä»¶
    for root, dirs, files in os.walk('.'):
        for file in files:
            if file.endswith(('.json',)) and 'training_dataset' in file:
                filepath = os.path.join(root, file)
                if filepath not in dataset_files:
                    dataset_files.append(filepath)
    
    return dataset_files

def validate_dataset_file(filepath: str) -> List[str]:
    """éªŒè¯å•ä¸ªæ•°æ®é›†æ–‡ä»¶"""
    errors = []
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # åŸºæœ¬schemaéªŒè¯
        validate(instance=data, schema=TRAINING_DATASET_SCHEMA)
        
        # é¢å¤–çš„ä¸šåŠ¡é€»è¾‘éªŒè¯
        errors.extend(validate_business_logic(data, filepath))
        
    except json.JSONDecodeError as e:
        errors.append(f"JSONæ ¼å¼é”™è¯¯: {e}")
    except ValidationError as e:
        errors.append(f"SchemaéªŒè¯é”™è¯¯: {e.message}")
    except FileNotFoundError:
        errors.append(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    except Exception as e:
        errors.append(f"æœªçŸ¥é”™è¯¯: {e}")
    
    return errors

def validate_business_logic(data: Dict[str, Any], filepath: str) -> List[str]:
    """éªŒè¯ä¸šåŠ¡é€»è¾‘"""
    errors = []
    
    meta = data.get('meta', {})
    dataset_index = data.get('dataset_index', [])
    
    # 1. éªŒè¯DAggeræ–‡ä»¶çš„training_typeå­—æ®µ
    if 'dagger' in filepath and meta.get('training_type') != 'dagger':
        errors.append("DAggeræ–‡ä»¶å¿…é¡»è®¾ç½®training_typeä¸º'dagger'")
    
    # 2. éªŒè¯statuså­—æ®µä¸€è‡´æ€§
    meta_status = meta.get('status')
    if meta_status == 'completed':
        # å¦‚æœmetaçŠ¶æ€ä¸ºcompletedï¼Œæ‰€æœ‰datasetåº”è¯¥ä¸ºproduced
        for idx, dataset in enumerate(dataset_index):
            if dataset.get('status') not in ['produced', None]:
                errors.append(f"æ•°æ®é›†ç´¢å¼•{idx}: metaçŠ¶æ€ä¸ºcompletedæ—¶ï¼Œæ‰€æœ‰æ•°æ®é›†çŠ¶æ€åº”ä¸ºproduced")
    
    # 3. éªŒè¯obs_pathä¸€è‡´æ€§
    for idx, dataset in enumerate(dataset_index):
        obs_path = dataset.get('obs_path', '')
        status = dataset.get('status', 'produced')
        
        if status == 'produced' and not obs_path:
            errors.append(f"æ•°æ®é›†ç´¢å¼•{idx}: çŠ¶æ€ä¸ºproducedæ—¶ï¼Œobs_pathä¸èƒ½ä¸ºç©º")
        elif status == 'pending' and obs_path:
            errors.append(f"æ•°æ®é›†ç´¢å¼•{idx}: çŠ¶æ€ä¸ºpendingæ—¶ï¼Œobs_pathåº”ä¸ºç©º")
    
    # 4. éªŒè¯ç‰ˆæœ¬å·ä¸€è‡´æ€§
    consumer_version = meta.get('consumer_version', '')
    dataset_version = meta.get('version', '')
    if consumer_version and dataset_version:
        # ç§»é™¤vå‰ç¼€è¿›è¡Œæ¯”è¾ƒ
        consumer_ver = consumer_version.lstrip('v')
        dataset_ver = dataset_version.lstrip('v')
        if consumer_ver != dataset_ver:
            errors.append(f"consumer_version({consumer_version})å’Œversion({dataset_version})ä¸ä¸€è‡´")
    
    # 5. éªŒè¯bundle_versionsæ ¼å¼
    bundle_versions = meta.get('bundle_versions', [])
    for bundle_version in bundle_versions:
        if not bundle_version.startswith('v'):
            errors.append(f"Bundleç‰ˆæœ¬{bundle_version}åº”ä»¥'v'å¼€å¤´")
    
    # 6. éªŒè¯æ•°æ®é›†åç§°å”¯ä¸€æ€§
    dataset_names = [ds.get('name') for ds in dataset_index]
    duplicates = [name for name in dataset_names if dataset_names.count(name) > 1]
    if duplicates:
        errors.append(f"æ•°æ®é›†åç§°é‡å¤: {set(duplicates)}")
    
    return errors

def main():
    """ä¸»å‡½æ•°"""
    print("=== æ•°æ®é›†æ ¼å¼éªŒè¯ ===")
    
    dataset_files = find_dataset_files()
    
    if not dataset_files:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®é›†æ–‡ä»¶")
        sys.exit(1)
    
    total_errors = 0
    
    for filepath in dataset_files:
        print(f"\nğŸ“ éªŒè¯æ–‡ä»¶: {filepath}")
        errors = validate_dataset_file(filepath)
        
        if errors:
            print(f"âŒ å‘ç° {len(errors)} ä¸ªé”™è¯¯:")
            for error in errors:
                print(f"   â€¢ {error}")
            total_errors += len(errors)
        else:
            print("âœ… æ ¼å¼éªŒè¯é€šè¿‡")
    
    print(f"\n=== éªŒè¯å®Œæˆ ===")
    print(f"æ£€æŸ¥æ–‡ä»¶æ•°: {len(dataset_files)}")
    print(f"æ€»é”™è¯¯æ•°: {total_errors}")
    
    if total_errors > 0:
        sys.exit(1)
    else:
        print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶éªŒè¯é€šè¿‡!")

if __name__ == "__main__":
    main()

