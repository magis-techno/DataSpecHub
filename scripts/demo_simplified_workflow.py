#!/usr/bin/env python3
"""
DataSpecHub å·¥ä½œæµç¨‹æ¼”ç¤º
å±•ç¤ºä»Consumeré…ç½®åˆ°æœ€ç»ˆæ•°æ®ç”Ÿäº§çš„å®Œæ•´æµç¨‹
"""

import sys
import json
from pathlib import Path
from datetime import datetime

def step1_consumer_configuration():
    """æ­¥éª¤1: æ‰‹åŠ¨é…ç½®Consumerç‰ˆæœ¬"""
    print("=" * 80)
    print("ğŸ¯ æ­¥éª¤1: Consumerç‰ˆæœ¬é…ç½®")
    print("=" * 80)
    
    # ä½¿ç”¨ä»“åº“ä¸­å·²æœ‰çš„Consumeré…ç½®
    consumer_path = "consumers/end_to_end/v1.2.0.yaml"
    print(f"ğŸ“‹ ä½¿ç”¨ç°æœ‰Consumeré…ç½®: {consumer_path}")
    
    # è¯»å–Consumeré…ç½®æ–‡ä»¶å†…å®¹ï¼ˆæ¨¡æ‹Ÿï¼‰
    sample_consumer = {
        "meta": {
            "consumer": "end_to_end",
            "version": "1.2.0",
            "description": "ç«¯åˆ°ç«¯ç½‘ç»œçš„æ•°æ®é€šé“ç‰ˆæœ¬éœ€æ±‚",
            "team": "ç«¯åˆ°ç«¯å›¢é˜Ÿ"
        },
        "requirements": [
            {"channel": "image_original", "version": "1.2.0", "required": True},
            {"channel": "object_array_fusion_infer", "version": ">=1.2.0", "required": True},
            {"channel": "occupancy", "version": "1.0.0", "required": True},
            {"channel": "utils_slam", "version": ">=1.0.0", "required": True}
        ]
    }
    
    print("\nâœ… Consumeré…ç½®å†…å®¹:")
    print(f"   Consumer: {sample_consumer['meta']['consumer']} v{sample_consumer['meta']['version']}")
    print(f"   å›¢é˜Ÿ: {sample_consumer['meta']['team']}")
    print(f"   ä¾èµ–é€šé“æ•°: {len(sample_consumer['requirements'])}")
    
    for req in sample_consumer['requirements']:
        print(f"      â€¢ {req['channel']}: {req['version']}")
    
    return sample_consumer

def step2_bundle_generation(consumer_config):
    """æ­¥éª¤2: è‡ªåŠ¨åŒ–ç”Ÿäº§Bundleæ–‡ä»¶"""
    print("\n" + "=" * 80)
    print("ğŸ”¨ æ­¥éª¤2: è‡ªåŠ¨åŒ–Bundleç”Ÿæˆ")
    print("=" * 80)
    
    # ç”ŸæˆBundleç‰ˆæœ¬å· (Consumerç‰ˆæœ¬-å¹´.å‘¨æ•°)
    current_time = datetime.now()
    week_number = current_time.isocalendar()[1]
    bundle_version = f"v{consumer_config['meta']['version']}-2025.{week_number}"
    
    # Bundleæ–‡ä»¶è·¯å¾„
    bundle_path = f"bundles/weekly/end_to_end-{bundle_version}.yaml"
    
    print(f"ğŸ¯ ç”ŸæˆBundleæ–‡ä»¶:")
    print(f"   Bundleç‰ˆæœ¬: {bundle_version}")
    print(f"   æ–‡ä»¶è·¯å¾„: {bundle_path}")
    
    # æ¨¡æ‹ŸBundleå†…å®¹
    bundle_content = {
        "meta": {
            "bundle_name": "end_to_end",
            "consumer_version": f"v{consumer_config['meta']['version']}",
            "bundle_version": bundle_version,
            "bundle_type": "weekly",
            "created_at": current_time.isoformat() + "Z",
            "description": f"ç«¯åˆ°ç«¯ç½‘ç»œæ•°æ®å¿«ç…§ - 2025å¹´ç¬¬{week_number}å‘¨"
        },
        "channels": []
    }
    
    # è§£æConsumerçº¦æŸï¼Œç”Ÿæˆå…·ä½“ç‰ˆæœ¬æ¸…å•
    print("\nğŸ” è§£æç‰ˆæœ¬çº¦æŸ:")
    for req in consumer_config['requirements']:
        channel = req['channel']
        constraint = req['version']
        
        # æ¨¡æ‹Ÿç‰ˆæœ¬è§£æç»“æœ
        if constraint.startswith(">="):
            available_versions = ["1.2.0", "1.1.0", "1.0.0"]
            recommended = "1.2.0"
        elif constraint == "1.2.0":
            available_versions = ["1.2.0"]
            recommended = "1.2.0"
        elif constraint == "1.0.0":
            available_versions = ["1.0.0"]
            recommended = "1.0.0"
        else:
            available_versions = ["1.1.0", "1.0.0"]
            recommended = "1.1.0"
        
        channel_spec = {
            "channel": channel,
            "available_versions": available_versions,
            "recommended_version": recommended,
            "source_constraint": constraint
        }
        
        bundle_content["channels"].append(channel_spec)
        print(f"   âœ… {channel}: {constraint} â†’ æ¨èç‰ˆæœ¬ {recommended}")
    
    print(f"\nğŸ“ Bundleæ–‡ä»¶å·²ç”Ÿæˆ: {bundle_path}")
    print(f"   åŒ…å« {len(bundle_content['channels'])} ä¸ªé€šé“è§„æ ¼")
    
    return bundle_content, bundle_path

def step3_training_dataset_generation(bundle_content):
    """æ­¥éª¤3: ç”Ÿæˆtraining_dataset.json"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æ­¥éª¤3: ç”ŸæˆTraining Dataseté…ç½®")
    print("=" * 80)
    
    bundle_version = bundle_content['meta']['bundle_version']
    
    # ç”Ÿæˆtraining_dataset.jsonå†…å®¹
    training_dataset = {
        "meta": {
            "dataset_name": "end_to_end_training_dataset",
            "bundle_version": bundle_version,  # è‡ªåŠ¨å¡«å†™bundleç‰ˆæœ¬
            "created_at": datetime.now().isoformat() + "Z",
            "description": f"åŸºäºBundle {bundle_version} çš„è®­ç»ƒæ•°æ®é›†é…ç½®"
        },
        "data_sources": [],
        "output_config": {
            "format": "jsonl",
            "output_path": f"datasets/end_to_end/{bundle_version}/",
            "split_strategy": "temporal",
            "train_ratio": 0.8,
            "val_ratio": 0.15,
            "test_ratio": 0.05
        }
    }
    
    # ä¸ºæ¯ä¸ªé€šé“é…ç½®æ•°æ®æº
    print(f"\nğŸ¯ é…ç½®æ•°æ®æº (Bundleç‰ˆæœ¬: {bundle_version}):")
    for channel_spec in bundle_content['channels']:
        channel = channel_spec['channel']
        version = channel_spec['recommended_version']
        
        data_source = {
            "channel": channel,
            "version": version,
            "data_path": f"data/{channel}/v{version}/",
            "file_pattern": "*.parquet",
            "sample_count": 50000 + hash(channel) % 20000,  # æ¨¡æ‹Ÿæ ·æœ¬æ•°
            "size_gb": 5.2 + hash(channel) % 10  # æ¨¡æ‹Ÿæ•°æ®å¤§å°
        }
        
        training_dataset["data_sources"].append(data_source)
        print(f"   âœ… {channel}@v{version}: {data_source['sample_count']:,} æ ·æœ¬, {data_source['size_gb']:.1f}GB")
    
    dataset_path = f"datasets/configs/training_dataset_{bundle_version.replace('.', '_')}.json"
    print(f"\nğŸ“ Training Dataseté…ç½®å·²ç”Ÿæˆ: {dataset_path}")
    
    return training_dataset, dataset_path

def step4_mock_data_production(training_dataset):
    """æ­¥éª¤4: Mockæ•°æ®ç”Ÿäº§è¿‡ç¨‹"""
    print("\n" + "=" * 80)
    print("ğŸ­ æ­¥éª¤4: Mockæ•°æ®ç”Ÿäº§è¿‡ç¨‹")
    print("=" * 80)
    
    bundle_version = training_dataset['meta']['bundle_version']
    output_path = training_dataset['output_config']['output_path']
    
    print(f"ğŸ¯ å¼€å§‹æ•°æ®ç”Ÿäº§æµç¨‹:")
    print(f"   Bundleç‰ˆæœ¬: {bundle_version}")
    print(f"   è¾“å‡ºè·¯å¾„: {output_path}")
    
    # æ¨¡æ‹Ÿæ•°æ®ç”Ÿäº§è¿‡ç¨‹
    total_samples = sum(ds['sample_count'] for ds in training_dataset['data_sources'])
    total_size = sum(ds['size_gb'] for ds in training_dataset['data_sources'])
    
    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples:,}")
    print(f"   æ€»æ•°æ®é‡: {total_size:.1f} GB")
    
    # æ¨¡æ‹Ÿç”Ÿæˆçš„JSONLæ–‡ä»¶
    jsonl_files = [
        f"{output_path}train.jsonl",
        f"{output_path}val.jsonl", 
        f"{output_path}test.jsonl"
    ]
    
    print(f"\nğŸ”„ ç”Ÿäº§JSONLæ–‡ä»¶:")
    splits = ['train', 'val', 'test']
    ratios = [0.8, 0.15, 0.05]
    
    for split, ratio, jsonl_file in zip(splits, ratios, jsonl_files):
        split_samples = int(total_samples * ratio)
        split_size = total_size * ratio
        
        print(f"   âœ… {jsonl_file}")
        print(f"      æ ·æœ¬æ•°: {split_samples:,} ({ratio*100:.0f}%)")
        print(f"      å¤§å°: {split_size:.1f} GB")
        
        # æ¨¡æ‹ŸJSONLæ–‡ä»¶å†…å®¹ç¤ºä¾‹
        if split == 'train':
            sample_jsonl = {
                "sample_id": "sample_001",
                "bundle_version": bundle_version,
                "timestamp": "2025-01-15T10:30:00Z",
                "channels": {
                    "image_original": {"path": "data/image_original/v1.2.0/frame_001.jpg", "metadata": {}},
                    "object_array_fusion_infer": {"path": "data/object_array_fusion_infer/v1.2.0/objects_001.json", "count": 5},
                    "occupancy": {"path": "data/occupancy/v1.0.0/grid_001.npy", "resolution": "0.1m"},
                    "utils_slam": {"path": "data/utils_slam/v1.1.0/pose_001.json", "confidence": 0.95}
                }
            }
            
            print(f"      ç¤ºä¾‹JSONLè®°å½•:")
            print(f"        sample_id: {sample_jsonl['sample_id']}")
            print(f"        bundle_version: {sample_jsonl['bundle_version']}")
            print(f"        channels: {len(sample_jsonl['channels'])} ä¸ª")
    
    print(f"\nğŸ‰ æ•°æ®ç”Ÿäº§å®Œæˆ!")
    print(f"   è¾“å‡ºç›®å½•: {output_path}")
    print(f"   ç”Ÿæˆæ–‡ä»¶: {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
    
    return jsonl_files

def workflow_summary():
    """å·¥ä½œæµç¨‹æ€»ç»“"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ DataSpecHub å·¥ä½œæµç¨‹æ€»ç»“")
    print("=" * 80)
    
    workflow_steps = [
        ("æ­¥éª¤1", "Consumeré…ç½®", "æ‰‹åŠ¨é…ç½®Consumerç‰ˆæœ¬å’Œä¾èµ–", "consumers/end_to_end/v1.2.0.yaml"),
        ("æ­¥éª¤2", "Bundleç”Ÿæˆ", "è‡ªåŠ¨è§£æçº¦æŸï¼Œç”Ÿæˆç‰ˆæœ¬æ¸…å•", "bundles/weekly/end_to_end-v1.2.0-2025.3.yaml"),
        ("æ­¥éª¤3", "Dataseté…ç½®", "åˆ›å»ºè®­ç»ƒæ•°æ®é›†é…ç½®æ–‡ä»¶", "datasets/configs/training_dataset_v1_2_0-2025_3.json"),
        ("æ­¥éª¤4", "æ•°æ®ç”Ÿäº§", "æ‰¹é‡ç”Ÿäº§JSONLæ ¼å¼è®­ç»ƒæ•°æ®", "datasets/end_to_end/v1.2.0-2025.3/")
    ]
    
    print(f"{'æ­¥éª¤':<8} {'åŠŸèƒ½':<12} {'è¯´æ˜':<24} {'è¾“å‡ºæ–‡ä»¶/è·¯å¾„':<40}")
    print("-" * 90)
    
    for step, function, description, output in workflow_steps:
        print(f"{step:<8} {function:<12} {description:<24} {output:<40}")
    
    print(f"\nğŸ¯ æ ¸å¿ƒä¼˜åŠ¿:")
    print(f"   âœ… äººå·¥é…ç½®Consumerï¼Œå‡å°‘è‡ªåŠ¨æ¨å¯¼å¤æ‚åº¦")
    print(f"   âœ… è‡ªåŠ¨åŒ–Bundleç”Ÿæˆï¼Œç¡®ä¿ç‰ˆæœ¬ä¸€è‡´æ€§")
    print(f"   âœ… ç»Ÿä¸€çš„Bundleç‰ˆæœ¬ç®¡ç†ï¼Œä¾¿äºè¿½æº¯")
    print(f"   âœ… æ ‡å‡†åŒ–çš„JSONLè¾“å‡ºï¼Œæ”¯æŒå¤šç§è®­ç»ƒæ¡†æ¶")
    
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹å¼:")
    print(f"   1. ä¿®æ”¹Consumeré…ç½®æ–‡ä»¶ï¼ˆæ‰‹åŠ¨ï¼‰")
    print(f"   2. è¿è¡Œ: python bundle_generator.py --consumer end_to_end")
    print(f"   3. è¿è¡Œ: python training_dataset_generator.py --bundle v1.2.0-2025.3")
    print(f"   4. è¿è¡Œ: python data_producer.py --dataset-config training_dataset.json")

def main():
    """ä¸»æ¼”ç¤ºæµç¨‹"""
    print("ğŸš€ DataSpecHub å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤º")
    print("å±•ç¤ºä»Consumeré…ç½®åˆ°æœ€ç»ˆæ•°æ®ç”Ÿäº§çš„ç«¯åˆ°ç«¯æµç¨‹")
    
    try:
        # æ­¥éª¤1: Consumeré…ç½®
        consumer_config = step1_consumer_configuration()
        
        # æ­¥éª¤2: Bundleç”Ÿæˆ
        bundle_content, bundle_path = step2_bundle_generation(consumer_config)
        
        # æ­¥éª¤3: Training Dataseté…ç½®
        training_dataset, dataset_path = step3_training_dataset_generation(bundle_content)
        
        # æ­¥éª¤4: Mockæ•°æ®ç”Ÿäº§
        jsonl_files = step4_mock_data_production(training_dataset)
        
        # å·¥ä½œæµç¨‹æ€»ç»“
        workflow_summary()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 80)
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ è¿™æ˜¯æ¼”ç¤ºè„šæœ¬ï¼ŒçœŸå®è¿è¡Œéœ€è¦å®Œæ•´çš„é¡¹ç›®ç¯å¢ƒ")

if __name__ == "__main__":
    main()
