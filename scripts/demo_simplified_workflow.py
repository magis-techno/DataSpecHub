#!/usr/bin/env python3
"""
DataSpecHub å·¥ä½œæµç¨‹æ¼”ç¤º
å±•ç¤ºä»Consumeré…ç½®åˆ°æœ€ç»ˆæ•°æ®ç”Ÿäº§çš„å®Œæ•´æµç¨‹
"""

import sys
import json
from pathlib import Path
from datetime import datetime

def step1_consumer_configuration():
    """æ­¥éª¤1: æ‰‹åŠ¨é…ç½®Consumerç‰ˆæœ¬"""
    print("=" * 80)
    print("ğŸ¯ æ­¥éª¤1: Consumerç‰ˆæœ¬é…ç½®")
    print("=" * 80)
    
    # ä½¿ç”¨ä»“åº“ä¸­å·²æœ‰çš„Consumeré…ç½®
    consumer_path = "consumers/end_to_end/v1.2.0.yaml"
    print(f"ğŸ“‹ ä½¿ç”¨ç°æœ‰Consumeré…ç½®: {consumer_path}")
    
    # è¯»å–Consumeré…ç½®æ–‡ä»¶å†…å®¹ï¼ˆæ¨¡æ‹Ÿï¼‰
    sample_consumer = {
        "meta": {
            "consumer": "end_to_end",
            "version": "1.2.0",
            "description": "ç«¯åˆ°ç«¯ç½‘ç»œçš„æ•°æ®é€šé“ç‰ˆæœ¬éœ€æ±‚",
            "team": "ç«¯åˆ°ç«¯å›¢é˜Ÿ"
        },
        "requirements": [
            {"channel": "image_original", "version": "1.2.0", "required": True},
            {"channel": "object_array_fusion_infer", "version": ">=1.2.0", "required": True},
            {"channel": "occupancy", "version": "1.0.0", "required": True},
            {"channel": "utils_slam", "version": ">=1.0.0", "required": True}
        ]
    }
    
    print("\nâœ… Consumeré…ç½®å†…å®¹:")
    print(f"   Consumer: {sample_consumer['meta']['consumer']} v{sample_consumer['meta']['version']}")
    print(f"   å›¢é˜Ÿ: {sample_consumer['meta']['team']}")
    print(f"   ä¾èµ–é€šé“æ•°: {len(sample_consumer['requirements'])}")
    
    for req in sample_consumer['requirements']:
        print(f"      â€¢ {req['channel']}: {req['version']}")
    
    return sample_consumer

def step2_bundle_generation(consumer_config):
    """æ­¥éª¤2: è‡ªåŠ¨åŒ–ç”Ÿäº§Bundleæ–‡ä»¶"""
    print("\n" + "=" * 80)
    print("ğŸ”¨ æ­¥éª¤2: è‡ªåŠ¨åŒ–Bundleç”Ÿæˆ")
    print("=" * 80)
    
    # ç”ŸæˆBundleç‰ˆæœ¬å· (Consumerç‰ˆæœ¬-å¹´.å‘¨æ•°)
    current_time = datetime.now()
    week_number = current_time.isocalendar()[1]
    bundle_version = f"v{consumer_config['meta']['version']}-2025.{week_number}"
    
    # Bundleæ–‡ä»¶è·¯å¾„
    bundle_path = f"bundles/weekly/end_to_end-{bundle_version}.yaml"
    
    print(f"ğŸ¯ ç”ŸæˆBundleæ–‡ä»¶:")
    print(f"   Bundleç‰ˆæœ¬: {bundle_version}")
    print(f"   æ–‡ä»¶è·¯å¾„: {bundle_path}")
    
    # æ¨¡æ‹ŸBundleå†…å®¹
    bundle_content = {
        "meta": {
            "bundle_name": "end_to_end",
            "consumer_version": f"v{consumer_config['meta']['version']}",
            "bundle_version": bundle_version,
            "bundle_type": "weekly",
            "created_at": current_time.isoformat() + "Z",
            "description": f"ç«¯åˆ°ç«¯ç½‘ç»œæ•°æ®å¿«ç…§ - 2025å¹´ç¬¬{week_number}å‘¨"
        },
        "channels": []
    }
    
    # è§£æConsumerçº¦æŸï¼Œç”Ÿæˆå…·ä½“ç‰ˆæœ¬æ¸…å•
    print("\nğŸ” è§£æç‰ˆæœ¬çº¦æŸ:")
    for req in consumer_config['requirements']:
        channel = req['channel']
        constraint = req['version']
        
        # æ¨¡æ‹Ÿç‰ˆæœ¬è§£æç»“æœ
        if constraint.startswith(">="):
            available_versions = ["1.2.0", "1.1.0", "1.0.0"]
            recommended = "1.2.0"
        elif constraint == "1.2.0":
            available_versions = ["1.2.0"]
            recommended = "1.2.0"
        elif constraint == "1.0.0":
            available_versions = ["1.0.0"]
            recommended = "1.0.0"
        else:
            available_versions = ["1.1.0", "1.0.0"]
            recommended = "1.1.0"
        
        channel_spec = {
            "channel": channel,
            "available_versions": available_versions,
            "recommended_version": recommended,
            "source_constraint": constraint
        }
        
        bundle_content["channels"].append(channel_spec)
        print(f"   âœ… {channel}: {constraint}")
        print(f"      æ¨èç‰ˆæœ¬: {recommended}")
        print(f"      å¯é€‰ç‰ˆæœ¬: {', '.join(available_versions)}")
    
    print(f"\nğŸ“ Bundleæ–‡ä»¶å·²ç”Ÿæˆ: {bundle_path}")
    print(f"   åŒ…å« {len(bundle_content['channels'])} ä¸ªé€šé“è§„æ ¼")
    
    return bundle_content, bundle_path

def step3_training_dataset_generation(bundle_content):
    """æ­¥éª¤3: ç”Ÿæˆtraining_dataset.json"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æ­¥éª¤3: ç”ŸæˆTraining Dataseté…ç½®")
    print("=" * 80)
    
    bundle_version = bundle_content['meta']['bundle_version']
    
    # ç”Ÿæˆtraining_dataset.jsonå†…å®¹
    training_dataset = {
        "meta": {
            "dataset_name": "end_to_end_training_dataset",
            "bundle_version": bundle_version,  # è‡ªåŠ¨å¡«å†™bundleç‰ˆæœ¬
            "created_at": datetime.now().isoformat() + "Z",
            "description": f"åŸºäºBundle {bundle_version} çš„è®­ç»ƒæ•°æ®é›†é…ç½®"
        },
        "data_sources": [],
        "output_config": {
            "format": "jsonl",
            "output_path": f"datasets/end_to_end/{bundle_version}/",
            "split_strategy": "temporal",
            "train_ratio": 0.8,
            "val_ratio": 0.15,
            "test_ratio": 0.05
        }
    }
    
    # ä¸ºæ¯ä¸ªé€šé“é…ç½®æ•°æ®æº
    print(f"\nğŸ¯ é…ç½®æ•°æ®æº (Bundleç‰ˆæœ¬: {bundle_version}):")
    for channel_spec in bundle_content['channels']:
        channel = channel_spec['channel']
        version = channel_spec['recommended_version']
        
        data_source = {
            "channel": channel,
            "version": version,
            "data_path": f"data/{channel}/v{version}/",
            "file_pattern": "*.parquet",
            "sample_count": 50000 + hash(channel) % 20000  # æ¨¡æ‹Ÿæ ·æœ¬æ•°
        }
        
        training_dataset["data_sources"].append(data_source)
        print(f"   âœ… {channel}@v{version}: {data_source['sample_count']:,} æ ·æœ¬")
    
    dataset_path = f"datasets/configs/training_dataset_production.json"
    print(f"\nğŸ“ Training Dataseté…ç½®å·²ç”Ÿæˆ: {dataset_path}")
    
    return training_dataset, dataset_path

def step4_mock_data_production(training_dataset):
    """æ­¥éª¤4: Mockæ•°æ®ç”Ÿäº§è¿‡ç¨‹"""
    print("\n" + "=" * 80)
    print("ğŸ­ æ­¥éª¤4: Mockæ•°æ®ç”Ÿäº§è¿‡ç¨‹")
    print("=" * 80)
    
    bundle_version = training_dataset['meta']['bundle_version']
    output_path = training_dataset['output_config']['output_path']
    
    print(f"ğŸ¯ å¼€å§‹æ•°æ®ç”Ÿäº§æµç¨‹:")
    print(f"   Bundleç‰ˆæœ¬: {bundle_version}")
    print(f"   è¾“å‡ºè·¯å¾„: {output_path}")
    
    # æ¨¡æ‹Ÿæ•°æ®ç”Ÿäº§è¿‡ç¨‹
    total_samples = sum(ds['sample_count'] for ds in training_dataset['data_sources'])
    
    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples:,}")
    print(f"   é€šé“æ•°é‡: {len(training_dataset['data_sources'])}")
    
    # æ¨¡æ‹Ÿç”Ÿæˆçš„JSONLæ–‡ä»¶
    jsonl_files = [
        f"{output_path}train.jsonl",
        f"{output_path}val.jsonl", 
        f"{output_path}test.jsonl"
    ]
    
    print(f"\nğŸ”„ ç”Ÿäº§JSONLæ–‡ä»¶:")
    splits = ['train', 'val', 'test']
    ratios = [0.8, 0.15, 0.05]
    
    for split, ratio, jsonl_file in zip(splits, ratios, jsonl_files):
        split_samples = int(total_samples * ratio)
        
        print(f"   âœ… {jsonl_file}")
        print(f"      æ ·æœ¬æ•°: {split_samples:,} ({ratio*100:.0f}%)")
        
        # æ¨¡æ‹ŸJSONLæ–‡ä»¶å†…å®¹ç¤ºä¾‹
        if split == 'train':
            sample_jsonl = {
                "sample_id": "sample_001",
                "bundle_version": bundle_version,
                "timestamp": "2025-01-15T10:30:00Z",
                "channels": {
                    "image_original": {"path": "data/image_original/v1.2.0/frame_001.jpg", "metadata": {}},
                    "object_array_fusion_infer": {"path": "data/object_array_fusion_infer/v1.2.0/objects_001.json", "count": 5},
                    "occupancy": {"path": "data/occupancy/v1.0.0/grid_001.npy", "resolution": "0.1m"},
                    "utils_slam": {"path": "data/utils_slam/v1.1.0/pose_001.json", "confidence": 0.95}
                }
            }
            
            print(f"      ç¤ºä¾‹JSONLè®°å½•:")
            print(f"        sample_id: {sample_jsonl['sample_id']}")
            print(f"        bundle_version: {sample_jsonl['bundle_version']}")
            print(f"        channels: {len(sample_jsonl['channels'])} ä¸ª")
    
    print(f"\nğŸ‰ æ•°æ®ç”Ÿäº§å®Œæˆ!")
    print(f"   è¾“å‡ºç›®å½•: {output_path}")
    print(f"   ç”Ÿæˆæ–‡ä»¶: {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
    
    return jsonl_files

def workflow_summary():
    """å·¥ä½œæµç¨‹æ€»ç»“"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ DataSpecHub å·¥ä½œæµç¨‹æ€»ç»“")
    print("=" * 80)
    
    workflow_steps = [
        ("æ­¥éª¤1", "Consumeré…ç½®", "æ‰‹åŠ¨é…ç½®Consumerç‰ˆæœ¬å’Œä¾èµ–", "consumers/end_to_end/v1.2.0.yaml"),
        ("æ­¥éª¤2", "Bundleç”Ÿæˆ", "è‡ªåŠ¨è§£æçº¦æŸï¼Œç”Ÿæˆç‰ˆæœ¬æ¸…å•", "bundles/weekly/end_to_end-v1.2.0-2025.3.yaml"),
        ("æ­¥éª¤3", "Dataseté…ç½®", "åˆ›å»ºè®­ç»ƒæ•°æ®é›†é…ç½®æ–‡ä»¶", "datasets/configs/training_dataset_v1_2_0-2025_3.json"),
        ("æ­¥éª¤4", "æ•°æ®ç”Ÿäº§", "æ‰¹é‡ç”Ÿäº§JSONLæ ¼å¼è®­ç»ƒæ•°æ®", "datasets/end_to_end/v1.2.0-2025.3/")
    ]
    
    print(f"{'æ­¥éª¤':<8} {'åŠŸèƒ½':<12} {'è¯´æ˜':<24} {'è¾“å‡ºæ–‡ä»¶/è·¯å¾„':<40}")
    print("-" * 90)
    
    for step, function, description, output in workflow_steps:
        print(f"{step:<8} {function:<12} {description:<24} {output:<40}")
    
    print(f"\nğŸ¯ æ ¸å¿ƒä¼˜åŠ¿:")
    print(f"   âœ… äººå·¥é…ç½®Consumerï¼Œå‡å°‘è‡ªåŠ¨æ¨å¯¼å¤æ‚åº¦")
    print(f"   âœ… è‡ªåŠ¨åŒ–Bundleç”Ÿæˆï¼Œç¡®ä¿ç‰ˆæœ¬ä¸€è‡´æ€§")
    print(f"   âœ… ç»Ÿä¸€çš„Bundleç‰ˆæœ¬ç®¡ç†ï¼Œä¾¿äºè¿½æº¯")
    print(f"   âœ… æ ‡å‡†åŒ–çš„JSONLè¾“å‡ºï¼Œæ”¯æŒå¤šç§è®­ç»ƒæ¡†æ¶")
    
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹å¼:")
    print(f"   1. ä¿®æ”¹Consumeré…ç½®æ–‡ä»¶ï¼ˆæ‰‹åŠ¨ï¼‰")
    print(f"   2. è¿è¡Œ: python bundle_generator.py --consumer end_to_end")
    print(f"   3. è¿è¡Œ: python training_dataset_generator.py --bundle v1.2.0-2025.3")
    print(f"   4. è¿è¡Œ: python data_producer.py --dataset-config training_dataset.json")

def export_to_markdown(consumer_config, bundle_content, training_dataset, jsonl_files):
    """å¯¼å‡ºæ¼”ç¤ºç»“æœä¸ºMarkdownæ ¼å¼"""
    bundle_version = bundle_content['meta']['bundle_version']
    
    markdown_content = f"""# DataSpecHub å·¥ä½œæµç¨‹æ¼”ç¤ºæŠ¥å‘Š

## æ¦‚è§ˆ
- **Consumer**: {consumer_config['meta']['consumer']} v{consumer_config['meta']['version']}
- **Bundleç‰ˆæœ¬**: {bundle_version}
- **ç”Ÿæˆæ—¶é—´**: {bundle_content['meta']['created_at']}
- **å›¢é˜Ÿ**: {consumer_config['meta']['team']}

## æ­¥éª¤1: Consumeré…ç½®

ä½¿ç”¨ç°æœ‰Consumeré…ç½®: `consumers/end_to_end/v{consumer_config['meta']['version']}.yaml`

### ä¾èµ–é€šé“
| é€šé“ | ç‰ˆæœ¬çº¦æŸ | æ˜¯å¦å¿…éœ€ |
|------|----------|----------|
"""
    
    for req in consumer_config['requirements']:
        required_text = "âœ…" if req['required'] else "âŒ"
        markdown_content += f"| {req['channel']} | {req['version']} | {required_text} |\n"
    
    markdown_content += f"""
## æ­¥éª¤2: Bundleç”Ÿæˆ

**Bundleæ–‡ä»¶è·¯å¾„**: `bundles/weekly/end_to_end-{bundle_version}.yaml`

### ç‰ˆæœ¬è§£æç»“æœ
| é€šé“ | çº¦æŸæ¡ä»¶ | æ¨èç‰ˆæœ¬ | å¯é€‰ç‰ˆæœ¬ |
|------|----------|----------|----------|
"""
    
    for channel_spec in bundle_content['channels']:
        available_str = ', '.join(channel_spec['available_versions'])
        markdown_content += f"| {channel_spec['channel']} | {channel_spec['source_constraint']} | {channel_spec['recommended_version']} | {available_str} |\n"
    
    total_samples = sum(ds['sample_count'] for ds in training_dataset['data_sources'])
    
    markdown_content += f"""
## æ­¥éª¤3: Training Dataseté…ç½®

**é…ç½®æ–‡ä»¶**: `datasets/configs/training_dataset_production.json`

### æ•°æ®æºé…ç½®
| é€šé“ | ç‰ˆæœ¬ | æ ·æœ¬æ•° | æ•°æ®è·¯å¾„ |
|------|------|--------|----------|
"""
    
    for ds in training_dataset['data_sources']:
        markdown_content += f"| {ds['channel']} | v{ds['version']} | {ds['sample_count']:,} | {ds['data_path']} |\n"
    
    markdown_content += f"""
## æ­¥éª¤4: æ•°æ®ç”Ÿäº§

**è¾“å‡ºè·¯å¾„**: `{training_dataset['output_config']['output_path']}`
**æ€»æ ·æœ¬æ•°**: {total_samples:,}
**é€šé“æ•°é‡**: {len(training_dataset['data_sources'])}

### ç”Ÿæˆæ–‡ä»¶
| æ–‡ä»¶ | æ ·æœ¬æ•° | æ¯”ä¾‹ |
|------|--------|------|
"""
    
    splits = ['train', 'val', 'test']
    ratios = [0.8, 0.15, 0.05]
    
    for split, ratio, jsonl_file in zip(splits, ratios, jsonl_files):
        split_samples = int(total_samples * ratio)
        markdown_content += f"| {jsonl_file} | {split_samples:,} | {ratio*100:.0f}% |\n"
    
    markdown_content += f"""
## å·¥ä½œæµç¨‹æ€»ç»“

### æ ¸å¿ƒä¼˜åŠ¿
- âœ… äººå·¥é…ç½®Consumerï¼Œå‡å°‘è‡ªåŠ¨æ¨å¯¼å¤æ‚åº¦
- âœ… è‡ªåŠ¨åŒ–Bundleç”Ÿæˆï¼Œç¡®ä¿ç‰ˆæœ¬ä¸€è‡´æ€§
- âœ… ç»Ÿä¸€çš„Bundleç‰ˆæœ¬ç®¡ç†ï¼Œä¾¿äºè¿½æº¯
- âœ… æ ‡å‡†åŒ–çš„JSONLè¾“å‡ºï¼Œæ”¯æŒå¤šç§è®­ç»ƒæ¡†æ¶

### ä½¿ç”¨å‘½ä»¤
1. ä¿®æ”¹Consumeré…ç½®æ–‡ä»¶ï¼ˆæ‰‹åŠ¨ï¼‰
2. `python bundle_generator.py --consumer end_to_end`
3. `python training_dataset_generator.py --bundle {bundle_version}`
4. `python data_producer.py --dataset-config training_dataset.json`

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {bundle_content['meta']['created_at']}*
"""
    
    return markdown_content

def main():
    """ä¸»æ¼”ç¤ºæµç¨‹"""
    print("ğŸš€ DataSpecHub å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤º")
    print("å±•ç¤ºä»Consumeré…ç½®åˆ°æœ€ç»ˆæ•°æ®ç”Ÿäº§çš„ç«¯åˆ°ç«¯æµç¨‹")
    
    try:
        # æ­¥éª¤1: Consumeré…ç½®
        consumer_config = step1_consumer_configuration()
        
        # æ­¥éª¤2: Bundleç”Ÿæˆ
        bundle_content, bundle_path = step2_bundle_generation(consumer_config)
        
        # æ­¥éª¤3: Training Dataseté…ç½®
        training_dataset, dataset_path = step3_training_dataset_generation(bundle_content)
        
        # æ­¥éª¤4: Mockæ•°æ®ç”Ÿäº§
        jsonl_files = step4_mock_data_production(training_dataset)
        
        # å·¥ä½œæµç¨‹æ€»ç»“
        workflow_summary()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 80)
        
        # è¯¢é—®æ˜¯å¦å¯¼å‡ºmarkdown
        print("\nğŸ’¡ æ˜¯å¦å¯¼å‡ºä¸ºMarkdownæŠ¥å‘Šï¼Ÿ")
        user_input = input("è¾“å…¥ 'y' æˆ– 'yes' å¯¼å‡ºmarkdownæŠ¥å‘Š (ç›´æ¥å›è½¦è·³è¿‡): ").lower().strip()
        
        if user_input in ['y', 'yes']:
            markdown_content = export_to_markdown(consumer_config, bundle_content, training_dataset, jsonl_files)
            markdown_filename = f"workflow_demo_report_{bundle_content['meta']['bundle_version'].replace('.', '_')}.md"
            
            try:
                with open(markdown_filename, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                print(f"âœ… MarkdownæŠ¥å‘Šå·²å¯¼å‡º: {markdown_filename}")
            except Exception as e:
                print(f"âŒ å¯¼å‡ºMarkdownæŠ¥å‘Šå¤±è´¥: {e}")
                print("ğŸ“‹ Markdownå†…å®¹é¢„è§ˆ:")
                print("-" * 50)
                print(markdown_content[:500] + "...")
        else:
            print("è·³è¿‡Markdownå¯¼å‡º")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ è¿™æ˜¯æ¼”ç¤ºè„šæœ¬ï¼ŒçœŸå®è¿è¡Œéœ€è¦å®Œæ•´çš„é¡¹ç›®ç¯å¢ƒ")

if __name__ == "__main__":
    main()
